# ğŸ‡»ğŸ‡³ ViLM: Vietnamese Language Model

**ViLM (Vietnamese Language Model)** is an end-to-end pipeline for building, training, and deploying Transformer-based language models specifically tailored for the Vietnamese language.

This repository provides the components to:

- ğŸ”¤ **Build** a custom tokenizer for Vietnamese
- ğŸ§  **Train** modern Transformer-based models
- âš¡ **Optimize and deploy** models with a high-performance C++ engine

---

## ğŸ“Œ Project Structure

```
vilm/
â”œâ”€â”€ tokenizer/      # Tokenizer building, training, and evaluation tools
â”‚   â”œâ”€â”€ data/           # Sample Vietnamese text for tokenizer training
â”‚   â””â”€â”€ ...             # Scripts (e.g., train_tokenizer.py)
â”œâ”€â”€ model/          # Model architecture definitions and training scripts
â”‚   â”œâ”€â”€ configs/        # YAML configuration files for training runs
â”‚   â””â”€â”€ ...             # Training scripts, model implementations
â”œâ”€â”€ inference/      # Optimized inference code and tools
â”‚   â”œâ”€â”€ c_runtime/      # C/C++ based inference engine
â”‚   â””â”€â”€ ...             # Optimization scripts, benchmarking tools
â”œâ”€â”€ data/           # (Optional) Larger Vietnamese corpora for model pre-training
â””â”€â”€ LICENSE         # Project license file
```

---

## ğŸ”¤ 1. [Build Tokenizer](https://github.com/vietnlp/vilm/tree/main/tokenizer)

We developed a custom tokenizer designed specifically for Vietnamese, addressing issues common with general-purpose tokenizers when applied to this language.

### ğŸš« Problems with Existing Tokenizers

- Split common Vietnamese words into 3â€“4 tokens
- Produce fragmented or meaningless subwords
- Poor handling of numerals, scientific terms, and special characters
- Include irrelevant foreign words in the vocabulary
- Lack optimization for Vietnamese grammar and syntax

### âœ… Advantages of Our Tokenizer

- **Reduced fragmentation** for better word representation
- **Fewer tokens per sentence**, improving model efficiency
- **Improved semantic accuracy** through meaningful tokens
- **Localized vocabulary** for Vietnamese language and culture

### âœ¨ Features

- **Collect & Analyze Corpora**
  - Curate high-quality Vietnamese datasets
  - Clean, deduplicate, and normalize text

- **Tokenizer Benchmarking**
  - Evaluate popular tokenizers:
    - General-purpose: GPT-4o, LLaMA 3, Gemma 2, Qwen 2.5
    - Vietnamese-specific: **PhoGPT**
  - Metrics: token count, vocab coverage, semantic coherence

- **Compare Tokenization Algorithms**
  - Subword methods supported:
    - BPE (Byte-Pair Encoding)
    - Unigram Language Model
    - WordPiece
    - Character-level and hybrid variants

- **Train Custom Tokenizer**
  - Based on [SentencePiece](https://github.com/google/sentencepiece)
  - Choose from `bpe`, `unigram`, etc.
  - Evaluate vocab size, token efficiency, and downstream performance

- **Vietnamese-specific Preprocessing**
  - Rule-based normalization for accents, compound words, etc.
  - Intelligent handling of abbreviations and scientific terms

- **Export Artifacts**
  - Save tokenizer model and configuration for reuse in training or inference

### ğŸ“š References

- [Pretraining LLMs â€“ DeepLearning.AI](https://www.deeplearning.ai/short-courses/pretraining-llms/)
- [The Role of Tokenizers â€“ DeepLearning.AI](https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization/)

### â–¶ï¸ How to Use

```bash
cd tokenizer
python train_tokenizer.py --input data/vietnamese_corpus.txt --model_type bpe --vocab_size 32000
```

---

## ğŸ§  2. [Build Model](https://github.com/vietnlp/vilm/tree/main/model)

This module handles model training using PyTorch. It supports baseline and experimental architectures for training from scratch on Vietnamese text.

### ğŸ§± Supported Architectures & Features

- Standard Decoder-Only Transformer
- Mixture-of-Experts (MoE) layers
- Mamba (State Space Models)
- Multi-head & Grouped-Query Attention (like LLaMA)
- Multi-latent attention (inspired by DeepSeek)
- Experimental: Diffusion-based LLMs

### ğŸ› ï¸ Training Workflow

- Scripts and configs for pretraining on large Vietnamese corpora
- Easily configurable architecture, hyperparameters, and training schemes
- Designed for extensibility and research

---

## âš¡ 3. [Inference](https://github.com/vietnlp/vilm/tree/main/inference)

This module optimizes and deploys trained models for fast, memory-efficient inference.

### ğŸš€ Optimization Techniques

- **FlashAttention** â€“ Accelerated attention computation
- **Quantization** â€“ Use AWQ to reduce memory usage with minimal accuracy loss
- **Knowledge Distillation** â€“ Smaller models mimic larger ones
- **Pruning** â€“ Remove redundant weights

### âš™ï¸ C++ Inference Engine

- Convert PyTorch models to C++ runtime using:
  - `llama.cpp`
  - `GGML`
  - Custom lightweight engines
- Target: ultra-low-latency deployment and edge compatibility

### ğŸ“Š Benchmarking

- Tools to measure model performance: latency, memory use, and throughput

---

## ğŸ¤ Contributing

We welcome contributions from the community! You can:

- ğŸ› Fix bugs or improve code
- ğŸŒŸ Add new features (e.g., new model types, tokenizer improvements)
- âš™ï¸ Enhance training or inference performance

Please open an issue or submit a PR to get involved.

---

## ğŸ“œ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for full details.

---

